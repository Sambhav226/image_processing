{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 115ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 5), found shape=(None, 6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# In the above layer, we have 6 as the vocabulary contains 6 words. \u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 2 represents the size of the embedding vector. \u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# input length is 5 as our sentences have 5 words length as input sequences.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m input_array3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m]), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m output_array3 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_array3\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#Size (1,6,2)\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m#Now we have each input encoded as a vector of size 2. \u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mprint\u001b[39m(output_array3[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m#First row for 0, second row for 1 and so on....\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/var/folders/rh/4qpmbm_s2xz6_r2zbh1h4rkh0000gn/T/__autograph_generated_file9d9ekrxn.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2440, in predict_function  *\n        return step_function(self, iterator)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2425, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2413, in run_step  **\n        outputs = model.predict_step(data)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/training.py\", line 2381, in predict_step\n        return self(x, training=False)\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/homebrew/lib/python3.11/site-packages/keras/src/engine/input_spec.py\", line 298, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 5), found shape=(None, 6)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "What is Embedding layer in keras?\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "#Each value in the input_array (of input dimension) is mapped to a vector \n",
    "#of a defined size (output_dimension).\n",
    "#Think of it as principal components where you map one dimension to another diemsnion(s).\n",
    "#Obviously, here we are doing PCA. Just mapping one vector to another with some initialization. \n",
    "# So a 1 X 10 array gives 1 X 10 X 50 array (input_dim=1x10, output_dim=50).\n",
    "# Embeddings can be randomly initialized on initialized using 0s or 1s. \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10, 50, embeddings_initializer=\"ones\")) #Try \"uniform\" and \"ones\" initializer\n",
    "\n",
    "#Array of size 10, --> should output size (10,1,50)\n",
    "input_array = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "output_array = model.predict(input_array)\n",
    "\n",
    "#Array of size (1,10) -->should output size (1,10,50)\n",
    "input_array2 = np.expand_dims(np.array([0,1,2,3,4,5,6,7,8,9]), axis=0)\n",
    "output_array2 = model.predict(input_array2)\n",
    "\n",
    "##############################################\n",
    "#Where in the world do we use embedding layer?\n",
    "\n",
    "#########################################\n",
    "\"\"\"\n",
    "Text processing (example: word2vec)\n",
    "Consider the following example sentences...\n",
    "\n",
    "Hello, How are you doing - [0, 1, 2, 3, 4] (Hello is 0, are is 1, etc.)\n",
    "Hello, How are you feeling - [0, 1, 2, 3, 5] (Feeling is 5)\n",
    "\n",
    "Now, we want to train a network with first layer as embedding layer. \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(6, 2, embeddings_initializer=\"uniform\", input_length=5))\n",
    "# In the above layer, we have 6 as the vocabulary contains 6 words. \n",
    "# 2 represents the size of the embedding vector. \n",
    "# input length is 5 as our sentences have 5 words length as input sequences.\n",
    "input_array3 = np.expand_dims(np.array([0,1,2,3,4,5]), axis=0)\n",
    "output_array3 = model.predict(input_array3) #Size (1,6,2)\n",
    "\n",
    "#Now we have each input encoded as a vector of size 2. \n",
    "print(output_array3[0]) #First row for 0, second row for 1 and so on....\n",
    "\n",
    "#Remember that these weights are part of the model training, so they get updated. \n",
    "\n",
    "#########################################\n",
    "#Conditional GAN where we provide labels as input.\n",
    "#We use embedding layer as the first layer to represent each class as a vector.\n",
    "#e.g., cifar10 dataset has 10 classes. We want these 10 classes to be represented\n",
    "#by vectors, each, that can be trained as part of discriminator training. \n",
    "#Let us say the vector would have a size 50. Then we can add an embedding layer\n",
    "#of size 50. \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(10, 50, embeddings_initializer=\"uniform\")) #Try \"uniform\" and \"ones\" initializer\n",
    "model.compile('rmsprop', 'mse')\n",
    "\n",
    "input_array = np.array([0,1,2,3,4,5,6,7,8,9])\n",
    "output_array = model.predict(input_array)\n",
    "print(output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
